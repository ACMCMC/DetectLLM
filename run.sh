python main.py --output_name main --base_model_name gpt2-xl --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2  --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name EleutherAI/gpt-neo-2.7B --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy' 
python main.py --output_name main --base_model_name EleutherAI/gpt-j-6B --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name facebook/opt-2.7b --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name facebook/opt-13b --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name decapoda-research/llama-13b-hf --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name EleutherAI/gpt-neox-20b --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'



python main.py --output_name main --base_model_name gpt2-xl --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset squad --dataset_key context --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name EleutherAI/gpt-neo-2.7B --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset squad --dataset_key context --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name EleutherAI/gpt-j-6B --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset squad --dataset_key context --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name facebook/opt-2.7b --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset squad --dataset_key context --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name facebook/opt-13b --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset squad --dataset_key context --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name decapoda-research/llama-13b-hf --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset squad --dataset_key context --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name EleutherAI/gpt-neox-20b --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset squad --dataset_key context --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'



python main.py --output_name main --base_model_name gpt2-xl --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset writing --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy' 
python main.py --output_name main --base_model_name EleutherAI/gpt-neo-2.7B --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset writing --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name EleutherAI/gpt-j-6B --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset writing --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name facebook/opt-2.7b --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset writing --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name facebook/opt-13b --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset writing --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name decapoda-research/llama-13b-hf --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50,100 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset writing --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'
python main.py --output_name main --base_model_name EleutherAI/gpt-neox-20b --batch_size 1 --mask_filling_model_name t5-3b --n_perturbation_list 1,10,20,50 --n_samples 300 --pct_words_masked 0.3 --span_length 2 --dataset writing --baselines 'rank,likelihood,logrank,LRR,DetectGPT,NPR,supervised_roberta_base,supervised_roberta_large,entropy'




